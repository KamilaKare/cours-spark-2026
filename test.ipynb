{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea2a778c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/02/20 10:09:10 WARN Utils: Your hostname, codespaces-e1abfc, resolves to a loopback address: 127.0.0.1; using 10.0.3.138 instead (on interface eth0)\n",
      "26/02/20 10:09:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/20 10:09:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|   Statut| OK|\n",
      "+---------+---+\n",
      "|Apprenant|  1|\n",
      "+---------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"CoursBigData\").getOrCreate()\n",
    "df = spark.createDataFrame([(\"Apprenant\", 1)], [\"Statut\", \"OK\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "347632ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/20 10:09:57 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|  Nom|Age|\n",
      "+-----+---+\n",
      "|Alice| 34|\n",
      "|  Bob| 45|\n",
      "+-----+---+\n",
      "\n",
      "✅ Si vous voyez le tableau ci-dessus, Spark est opérationnel !\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 1. Création de la session (C'est ici que ça plante d'habitude en local !)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TestCoursBigData\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2. Création de données factices\n",
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Charlie\", 29)]\n",
    "df = spark.createDataFrame(data, [\"Nom\", \"Age\"])\n",
    "\n",
    "# 3. Une petite transformation\n",
    "df.filter(df.Age > 30).show()\n",
    "\n",
    "print(\"✅ Si vous voyez le tableau ci-dessus, Spark est opérationnel !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32e27940",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/20 10:11:16 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients : [1.1733726232939785,9.787407977673896]\n",
      "Interception : -48.59644151949395\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# 1. Préparation des données (Format Spark ML : Label + Features en vecteur)\n",
    "data = spark.createDataFrame([\n",
    "    (100.0, 2.0, 15.0),\n",
    "    (200.0, 3.0, 25.0),\n",
    "    (300.0, 5.0, 35.0)], \n",
    "    [\"label\", \"feature1\", \"feature2\"])\n",
    "\n",
    "# Spark ML a besoin que toutes les colonnes d'entrée soient regroupées dans un seul vecteur\n",
    "assembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\"], outputCol=\"features\")\n",
    "training_data = assembler.transform(data)\n",
    "\n",
    "# 2. Entraînement du modèle\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(training_data)\n",
    "\n",
    "# 3. Résultat\n",
    "print(f\"Coefficients : {lr_model.coefficients}\")\n",
    "print(f\"Interception : {lr_model.intercept}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1baf0866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargement de MNIST... (un peu de patience)\n",
      "Fini !\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "\n",
    "# Téléchargement d'un extrait MNIST train en CSV depuis une source stable\n",
    "if not os.path.exists(\"mnist_train.csv\"):\n",
    "    npz_url = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\"\n",
    "    npz_path = \"mnist.npz\"\n",
    "\n",
    "    print(\"Téléchargement de MNIST... (un peu de patience)\")\n",
    "    urllib.request.urlretrieve(npz_url, npz_path)\n",
    "\n",
    "    with np.load(npz_path) as data:\n",
    "        x_train = data[\"x_train\"].reshape(-1, 28 * 28)\n",
    "        y_train = data[\"y_train\"].reshape(-1, 1)\n",
    "\n",
    "    mnist_train = np.hstack([y_train, x_train])\n",
    "    np.savetxt(\"mnist_train.csv\", mnist_train, fmt=\"%d\", delimiter=\",\")\n",
    "\n",
    "    os.remove(npz_path)\n",
    "    print(\"Fini !\")\n",
    "else:\n",
    "    print(\"mnist_train.csv existe déjà.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a35a5aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/02/20 14:04:34 WARN Utils: Your hostname, codespaces-e1abfc, resolves to a loopback address: 127.0.0.1; using 10.0.0.88 instead (on interface eth0)\n",
      "26/02/20 14:04:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/20 14:04:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Assure une session Spark active même si les cellules précédentes n'ont pas été exécutées\n",
    "spark = SparkSession.builder.appName(\"MNIST_Preparation\").getOrCreate()\n",
    "\n",
    "# Chargement du CSV\n",
    "# On suppose que la colonne 0 est le \"label\" et les autres les pixels\n",
    "raw_data = spark.read.csv(\"mnist_train.csv\", header=False, inferSchema=True)\n",
    "\n",
    "# Renommer la première colonne en 'label'\n",
    "cols = raw_data.columns\n",
    "data = raw_data.withColumnRenamed(cols[0], \"label\")\n",
    "\n",
    "# Regrouper les 784 colonnes de pixels (features) dans un seul vecteur\n",
    "assembler = VectorAssembler(inputCols=cols[1:], outputCol=\"features\")\n",
    "final_data = assembler.transform(data).select(\"label\", \"features\")\n",
    "\n",
    "# Split Train/Test\n",
    "(train_data, test_data) = final_data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e38cc63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/20 14:05:52 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 19:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision Arbre de Décision : 67.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Création du modèle\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "dt_model = dt.fit(train_data)\n",
    "\n",
    "# Prédictions\n",
    "dt_predictions = dt_model.transform(test_data)\n",
    "\n",
    "# Évaluation\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(dt_predictions)\n",
    "print(f\"Précision Arbre de Décision : {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08568606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/20 14:08:17 WARN DAGScheduler: Broadcasting large task binary with size 1051.3 KiB\n",
      "[Stage 38:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision Random Forest : 83.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Création du modèle Random Forest\n",
    "# numTrees=20 est un bon compromis pour le Codespace\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=20)\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# Prédictions\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "\n",
    "# Évaluation\n",
    "rf_accuracy = evaluator.evaluate(rf_predictions)\n",
    "print(f\"Précision Random Forest : {rf_accuracy:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
